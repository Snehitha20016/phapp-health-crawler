{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6a53be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Set\n",
    "import argparse\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78b6378e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved to: C:\\Users\\snehi\\OneDrive\\Desktop\\New folder\\PHIT\\cleaned_us_ca.csv\n",
      "  - Rows: 62\n",
      "  - Columns: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\snehi\\AppData\\Local\\Temp\\ipykernel_18740\\76467755.py:8: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "RAW_PATH = r\"C:\\Users\\snehi\\OneDrive\\Desktop\\New folder\\PHIT\\us-ca.csv\"\n",
    "OUTPUT_PATH = r\"C:\\Users\\snehi\\OneDrive\\Desktop\\New folder\\PHIT\\cleaned_us_ca.csv\"  \n",
    "\n",
    "\n",
    "def clean_us_ca(input_path: str, output_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(input_path, delimiter=\";\", dtype=str)\n",
    "\n",
    "    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "    if \"pha\" in df.columns:\n",
    "        df[\"pha\"] = (\n",
    "            df[\"pha\"]\n",
    "            .astype(str)\n",
    "            .str.strip()\n",
    "            .str.upper()\n",
    "            .map({\"TRUE\": True, \"FALSE\": False})\n",
    "        )\n",
    "\n",
    "    if \"population_proper\" in df.columns:\n",
    "        df[\"population_proper\"] = pd.to_numeric(\n",
    "            df[\"population_proper\"], errors=\"coerce\"\n",
    "        ).astype(\"Int64\")\n",
    "\n",
    "    if \"state_id\" in df.columns:\n",
    "        df[\"state_id\"] = df[\"state_id\"].str.upper()\n",
    "\n",
    "    critical_cols = [c for c in [\"name\", \"pha_url\"] if c in df.columns]\n",
    "    if critical_cols:\n",
    "        df = df.dropna(subset=critical_cols)\n",
    "        for c in critical_cols:\n",
    "            df = df[df[c].astype(str).str.strip() != \"\"]\n",
    "\n",
    "    if \"community_id\" in df.columns:\n",
    "        df = df.drop_duplicates(subset=[\"community_id\"])\n",
    "    else:\n",
    "        subset_cols = [c for c in [\"name\", \"state_id\"] if c in df.columns]\n",
    "        if subset_cols:\n",
    "            df = df.drop_duplicates(subset=subset_cols)\n",
    "\n",
    "    sort_cols = [c for c in [\"state_id\", \"name\"] if c in df.columns]\n",
    "    if sort_cols:\n",
    "        df = df.sort_values(sort_cols)\n",
    "\n",
    "    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Cleaned dataset saved to: {output_path}\")\n",
    "    print(f\"  - Rows: {len(df)}\")\n",
    "    print(f\"  - Columns: {len(df.columns)}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clean_us_ca(RAW_PATH, OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce785f1b",
   "metadata": {},
   "source": [
    "Web Scraping Fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "588c0247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample HTML for testing (simulates a health department website)\n",
    "SAMPLE_HTML = \"\"\"\n",
    "<html>\n",
    "<head><title>County Health Department</title></head>\n",
    "<body>\n",
    "    <div class=\"contact-section\">\n",
    "        <h1>Butte County Public Health</h1>\n",
    "        <div class=\"contact-info\">\n",
    "            <p>Main Office: (530) 552-3800</p>\n",
    "            <p>Email: publichealth@buttecounty.net</p>\n",
    "            <p>Crisis Line: 1-800-334-6622</p>\n",
    "        </div>\n",
    "        <div class=\"address\">\n",
    "            <p>Address: 202 Mira Loma Drive, Oroville, CA 95965</p>\n",
    "        </div>\n",
    "    </div>\n",
    "    <div class=\"services\">\n",
    "        <h2>Services</h2>\n",
    "        <ul>\n",
    "            <li>COVID-19 Vaccination and Testing</li>\n",
    "            <li>Immunization Services</li>\n",
    "            <li>Maternal and Child Health</li>\n",
    "            <li>Mental Health Services</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c323d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_webpage(url):\n",
    "    \"\"\"\n",
    "    Fetch a webpage using requests library\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL to fetch\n",
    "        \n",
    "    Returns:\n",
    "        str: HTML content of the page, or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Fetching: {url}\")\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()  \n",
    "        print(f\"Successfully fetched {url} (Status: {response.status_code})\")\n",
    "        return response.text\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0044a7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html(html_content):\n",
    "    \"\"\"\n",
    "    Parse HTML content using BeautifulSoup\n",
    "    \n",
    "    Args:\n",
    "        html_content (str): Raw HTML string\n",
    "        \n",
    "    Returns:\n",
    "        BeautifulSoup: Parsed HTML object\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3998bdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_phone_numbers(soup):\n",
    "    \"\"\"\n",
    "    Extract phone numbers using regex patterns\n",
    "    \n",
    "    Args:\n",
    "        soup (BeautifulSoup): Parsed HTML\n",
    "        \n",
    "    Returns:\n",
    "        list: List of phone numbers found\n",
    "    \"\"\"\n",
    "    \n",
    "    phone_pattern = re.compile(\n",
    "        r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}|'  \n",
    "        r'1[-.\\s]?\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}'  \n",
    "    )\n",
    "    \n",
    "    text = soup.get_text()\n",
    "    \n",
    "    phones = phone_pattern.findall(text)\n",
    "    \n",
    "    unique_phones = list(dict.fromkeys(phones))\n",
    "    \n",
    "    return unique_phones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84bcfcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emails(soup):\n",
    "    \"\"\"\n",
    "    Extract email addresses using regex\n",
    "    \n",
    "    Args:\n",
    "        soup (BeautifulSoup): Parsed HTML\n",
    "        \n",
    "    Returns:\n",
    "        list: List of email addresses found\n",
    "    \"\"\"\n",
    "    email_pattern = re.compile(\n",
    "        r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "    )\n",
    "    \n",
    "    text = soup.get_text()\n",
    "    emails = email_pattern.findall(text)\n",
    "    \n",
    "    return list(dict.fromkeys(emails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1101e4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_addresses(soup):\n",
    "    \"\"\"\n",
    "    Extract physical addresses using regex\n",
    "    \n",
    "    Args:\n",
    "        soup (BeautifulSoup): Parsed HTML\n",
    "        \n",
    "    Returns:\n",
    "        list: List of addresses found\n",
    "    \"\"\"\n",
    "    \n",
    "    address_pattern = re.compile(\n",
    "        r'\\d+\\s+[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\s+'  \n",
    "        r'(?:Street|St|Avenue|Ave|Road|Rd|Drive|Dr|Lane|Ln|Boulevard|Blvd|Way)\\.?'\n",
    "        r'(?:,?\\s+[A-Z][a-z]+,?\\s+[A-Z]{2}\\s+\\d{5})?',  \n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    text = soup.get_text()\n",
    "    addresses = address_pattern.findall(text)\n",
    "    \n",
    "    return addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3193d3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_with_css_selectors(soup):\n",
    "    \"\"\"\n",
    "    Extract data using CSS selectors (alternative method)\n",
    "    \n",
    "    Args:\n",
    "        soup (BeautifulSoup): Parsed HTML\n",
    "        \n",
    "    Returns:\n",
    "        dict: Extracted data organized by type\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        'title': None,\n",
    "        'contact_info': [],\n",
    "        'services': []\n",
    "    }\n",
    "    \n",
    "    title_tag = soup.select_one('h1')\n",
    "    if title_tag:\n",
    "        data['title'] = title_tag.text.strip()\n",
    "    \n",
    "    contact_paragraphs = soup.select('.contact-info p')\n",
    "    data['contact_info'] = [p.text.strip() for p in contact_paragraphs]\n",
    "    \n",
    "    service_items = soup.select('.services li')\n",
    "    data['services'] = [li.text.strip() for li in service_items]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90214fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_beautifulsoup_methods(soup):\n",
    "    \"\"\"\n",
    "    Demonstrate common BeautifulSoup methods\n",
    "    \n",
    "    Args:\n",
    "        soup (BeautifulSoup): Parsed HTML\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BeautifulSoup Methods Demonstration\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    h1 = soup.find('h1')\n",
    "    print(f\"\\n.find('h1'): {h1.text if h1 else 'Not found'}\")\n",
    "    \n",
    "    paragraphs = soup.find_all('p')\n",
    "    print(f\"\\n.find_all('p'): Found {len(paragraphs)} paragraphs\")\n",
    "    for i, p in enumerate(paragraphs[:3], 1):\n",
    "        print(f\"  {i}. {p.text.strip()}\")\n",
    "    \n",
    "    contact_divs = soup.select('.contact-info')\n",
    "    print(f\"\\n.select('.contact-info'): Found {len(contact_divs)} elements\")\n",
    "    \n",
    "    text_sample = soup.get_text()[:100]\n",
    "    print(f\"\\n.get_text() sample: {text_sample}...\")\n",
    "    \n",
    "    title_tag = soup.find('title')\n",
    "    if title_tag:\n",
    "        print(f\"\\ntitle tag content: {title_tag.string}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "350ba112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BeautifulSoup Methods Demonstration\n",
      "============================================================\n",
      "\n",
      ".find('h1'): Butte County Public Health\n",
      "\n",
      ".find_all('p'): Found 4 paragraphs\n",
      "  1. Main Office: (530) 552-3800\n",
      "  2. Email: publichealth@buttecounty.net\n",
      "  3. Crisis Line: 1-800-334-6622\n",
      "\n",
      ".select('.contact-info'): Found 1 elements\n",
      "\n",
      ".get_text() sample: \n",
      "\n",
      "County Health Department\n",
      "\n",
      "\n",
      "Butte County Public Health\n",
      "\n",
      "Main Office: (530) 552-3800\n",
      "Email: publiche...\n",
      "\n",
      "title tag content: County Health Department\n",
      "\n",
      "============================================================\n",
      "Data Extraction with Regex\n",
      "============================================================\n",
      "\n",
      " Phone Numbers Found: 2\n",
      "  - (530) 552-3800\n",
      "  - 1-800-334-6622\n",
      "\n",
      " Emails Found: 1\n",
      "  - publichealth@buttecounty.net\n",
      "\n",
      " Addresses Found: 1\n",
      "  - 202 Mira Loma Drive, Oroville, CA 95965\n",
      "\n",
      "============================================================\n",
      "Data Extraction with CSS Selectors\n",
      "============================================================\n",
      "\n",
      " Facility: Butte County Public Health\n",
      "\n",
      " Contact Info (3 items):\n",
      "  - Main Office: (530) 552-3800\n",
      "  - Email: publichealth@buttecounty.net\n",
      "  - Crisis Line: 1-800-334-6622\n",
      "\n",
      " Services (4 items):\n",
      "  - COVID-19 Vaccination and Testing\n",
      "  - Immunization Services\n",
      "  - Maternal and Child Health\n",
      "  - Mental Health Services\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to demonstrate web scraping techniques\"\"\"\n",
    "    \n",
    "    soup = parse_html(SAMPLE_HTML)\n",
    "        \n",
    "    demonstrate_beautifulsoup_methods(soup)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Data Extraction with Regex\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    phones = extract_phone_numbers(soup)\n",
    "    print(f\"\\n Phone Numbers Found: {len(phones)}\")\n",
    "    for phone in phones:\n",
    "        print(f\"  - {phone}\")\n",
    "    \n",
    "    emails = extract_emails(soup)\n",
    "    print(f\"\\n Emails Found: {len(emails)}\")\n",
    "    for email in emails:\n",
    "        print(f\"  - {email}\")\n",
    "    \n",
    "    addresses = extract_addresses(soup)\n",
    "    print(f\"\\n Addresses Found: {len(addresses)}\")\n",
    "    for address in addresses:\n",
    "        print(f\"  - {address}\")\n",
    "    \n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Data Extraction with CSS Selectors\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    css_data = extract_with_css_selectors(soup)\n",
    "    print(f\"\\n Facility: {css_data['title']}\")\n",
    "    print(f\"\\n Contact Info ({len(css_data['contact_info'])} items):\")\n",
    "    for info in css_data['contact_info']:\n",
    "        print(f\"  - {info}\")\n",
    "    print(f\"\\n Services ({len(css_data['services'])} items):\")\n",
    "    for service in css_data['services']:\n",
    "        print(f\"  - {service}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858205ee",
   "metadata": {},
   "source": [
    "Building Python Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b0eb56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HealthCrawler:\n",
    "    \"\"\"\n",
    "    Web crawler for extracting public health resources from websites\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, delay: float = 2.0):\n",
    "        \"\"\"\n",
    "        Initialize the crawler\n",
    "        \n",
    "        Args:\n",
    "            delay (float): Delay in seconds between requests (default: 2.0)\n",
    "        \"\"\"\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'PHapp-HealthCrawler/1.0 (Educational Project)'\n",
    "        })\n",
    "        self.delay = delay\n",
    "        \n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        self.phone_pattern = re.compile(\n",
    "            r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}|'\n",
    "            r'1[-.\\s]?\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}'\n",
    "        )\n",
    "        self.email_pattern = re.compile(\n",
    "            r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "        )\n",
    "        self.address_pattern = re.compile(\n",
    "            r'\\d+\\s+[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\s+'\n",
    "            r'(?:Street|St|Avenue|Ave|Road|Rd|Drive|Dr|Lane|Ln|Boulevard|Blvd|Way)\\.?'\n",
    "            r'(?:,?\\s+[A-Z][a-z]+,?\\s+[A-Z]{2}\\s+\\d{5})?',\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "    \n",
    "    def fetch_page(self, url: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Fetch a webpage with error handling\n",
    "        \n",
    "        Args:\n",
    "            url (str): URL to fetch\n",
    "            \n",
    "        Returns:\n",
    "            str: HTML content or None if error\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Fetching: {url}\")\n",
    "            response = self.session.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            self.logger.info(f\"✓ Success: {url} (Status: {response.status_code})\")\n",
    "            return response.text\n",
    "            \n",
    "        except requests.Timeout:\n",
    "            self.logger.error(f\"✗ Timeout: {url}\")\n",
    "            return None\n",
    "            \n",
    "        except requests.ConnectionError:\n",
    "            self.logger.error(f\"✗ Connection error: {url}\")\n",
    "            return None\n",
    "            \n",
    "        except requests.HTTPError as e:\n",
    "            self.logger.error(f\"✗ HTTP error: {url} - {e}\")\n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"✗ Unexpected error: {url} - {e}\")\n",
    "            return None\n",
    "        \n",
    "        finally:\n",
    "            time.sleep(self.delay)\n",
    "    \n",
    "    def parse_html(self, html_content: str) -> BeautifulSoup:\n",
    "        \"\"\"\n",
    "        Parse HTML content\n",
    "        \n",
    "        Args:\n",
    "            html_content (str): Raw HTML\n",
    "            \n",
    "        Returns:\n",
    "            BeautifulSoup: Parsed HTML object\n",
    "        \"\"\"\n",
    "        return BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    def extract_phones(self, soup: BeautifulSoup) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract phone numbers from parsed HTML\n",
    "        \n",
    "        Args:\n",
    "            soup (BeautifulSoup): Parsed HTML\n",
    "            \n",
    "        Returns:\n",
    "            list: Unique phone numbers found\n",
    "        \"\"\"\n",
    "        text = soup.get_text()\n",
    "        phones = self.phone_pattern.findall(text)\n",
    "        return list(dict.fromkeys(phones)) \n",
    "    \n",
    "    def extract_emails(self, soup: BeautifulSoup) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract email addresses from parsed HTML\n",
    "        \n",
    "        Args:\n",
    "            soup (BeautifulSoup): Parsed HTML\n",
    "            \n",
    "        Returns:\n",
    "            list: Unique email addresses found\n",
    "        \"\"\"\n",
    "        text = soup.get_text()\n",
    "        emails = self.email_pattern.findall(text)\n",
    "        return list(dict.fromkeys(emails))\n",
    "    \n",
    "    def extract_addresses(self, soup: BeautifulSoup) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract physical addresses from parsed HTML\n",
    "        \n",
    "        Args:\n",
    "            soup (BeautifulSoup): Parsed HTML\n",
    "            \n",
    "        Returns:\n",
    "            list: Addresses found\n",
    "        \"\"\"\n",
    "        text = soup.get_text()\n",
    "        addresses = self.address_pattern.findall(text)\n",
    "        return addresses\n",
    "    \n",
    "    def extract_title(self, soup: BeautifulSoup) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Extract page title or main heading\n",
    "        \n",
    "        Args:\n",
    "            soup (BeautifulSoup): Parsed HTML\n",
    "            \n",
    "        Returns:\n",
    "            str: Title or None\n",
    "        \"\"\"\n",
    "        h1 = soup.find('h1')\n",
    "        if h1:\n",
    "            return h1.text.strip()\n",
    "        \n",
    "        title = soup.find('title')\n",
    "        if title:\n",
    "            return title.string.strip()\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def extract_data(self, soup: BeautifulSoup, url: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Extract all relevant data from parsed HTML\n",
    "        \n",
    "        Args:\n",
    "            soup (BeautifulSoup): Parsed HTML\n",
    "            url (str): Source URL\n",
    "            \n",
    "        Returns:\n",
    "            dict: Extracted data\n",
    "        \"\"\"\n",
    "        data = {\n",
    "            'url': url,\n",
    "            'title': self.extract_title(soup),\n",
    "            'phones': self.extract_phones(soup),\n",
    "            'emails': self.extract_emails(soup),\n",
    "            'addresses': self.extract_addresses(soup),\n",
    "            'scraped_at': datetime.now().isoformat(),\n",
    "            'success': True\n",
    "        }\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def crawl(self, url: str) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        Main crawl method - fetch and extract data from a URL\n",
    "        \n",
    "        Args:\n",
    "            url (str): URL to crawl\n",
    "            \n",
    "        Returns:\n",
    "            dict: Extracted data or None if failed\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Starting crawl: {url}\")\n",
    "        \n",
    "        html_content = self.fetch_page(url)\n",
    "        if not html_content:\n",
    "            return {\n",
    "                'url': url,\n",
    "                'success': False,\n",
    "                'error': 'Failed to fetch page',\n",
    "                'scraped_at': datetime.now().isoformat()\n",
    "            }\n",
    "        \n",
    "        soup = self.parse_html(html_content)\n",
    "        \n",
    "        data = self.extract_data(soup, url)\n",
    "        \n",
    "        self.logger.info(f\"✓ Crawl complete: {url}\")\n",
    "        self.logger.info(f\"  Found: {len(data['phones'])} phones, \"\n",
    "                        f\"{len(data['emails'])} emails, \"\n",
    "                        f\"{len(data['addresses'])} addresses\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def batch_crawl(self, urls: List[str]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Crawl multiple URLs\n",
    "        \n",
    "        Args:\n",
    "            urls (list): List of URLs to crawl\n",
    "            \n",
    "        Returns:\n",
    "            list: List of extracted data dictionaries\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        total = len(urls)\n",
    "        \n",
    "        self.logger.info(f\"Starting batch crawl of {total} URLs\")\n",
    "        \n",
    "        for i, url in enumerate(urls, 1):\n",
    "            self.logger.info(f\"\\n[{i}/{total}] Processing: {url}\")\n",
    "            data = self.crawl(url)\n",
    "            if data:\n",
    "                results.append(data)\n",
    "        \n",
    "        self.logger.info(f\"\\n✓ Batch crawl complete: {len(results)}/{total} successful\")\n",
    "        return results\n",
    "    \n",
    "    def save_to_json(self, data: List[Dict], filename: str):\n",
    "        \"\"\"\n",
    "        Save extracted data to JSON file\n",
    "        \n",
    "        Args:\n",
    "            data (list): List of data dictionaries\n",
    "            filename (str): Output filename\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "            self.logger.info(f\"✓ Saved data to {filename}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"✗ Error saving to {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c6184e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 16:34:46,627 - INFO - Starting crawl: http://www.acphd.org\n",
      "2025-12-08 16:34:46,627 - INFO - Fetching: http://www.acphd.org\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample URLs:\n",
      "  - https://www.buttecounty.net/610/Public-Health\n",
      "  - https://www.placer.ca.gov/2863/Public-Health\n",
      "  - https://dhs.saccounty.net/PUB/Pages/PUB-Home.asp\n",
      "\n",
      "============================================================\n",
      "Single URL Crawl Example\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 16:34:46,882 - INFO - ✓ Success: http://www.acphd.org (Status: 200)\n",
      "2025-12-08 16:34:48,917 - INFO - ✓ Crawl complete: http://www.acphd.org\n",
      "2025-12-08 16:34:48,917 - INFO -   Found: 2 phones, 0 emails, 2 addresses\n",
      "2025-12-08 16:34:48,917 - INFO - Starting batch crawl of 3 URLs\n",
      "2025-12-08 16:34:48,917 - INFO - \n",
      "[1/3] Processing: http://www.acphd.org\n",
      "2025-12-08 16:34:48,917 - INFO - Starting crawl: http://www.acphd.org\n",
      "2025-12-08 16:34:48,917 - INFO - Fetching: http://www.acphd.org\n",
      "2025-12-08 16:34:49,002 - INFO - ✓ Success: http://www.acphd.org (Status: 200)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully crawled: http://www.acphd.org\n",
      "  Title: Alameda County Public Health Department\n",
      "  Phones: 2\n",
      "  Emails: 0\n",
      "  Addresses: 2\n",
      "\n",
      "============================================================\n",
      "Batch Crawl Example\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 16:34:51,051 - INFO - ✓ Crawl complete: http://www.acphd.org\n",
      "2025-12-08 16:34:51,053 - INFO -   Found: 2 phones, 0 emails, 2 addresses\n",
      "2025-12-08 16:34:51,054 - INFO - \n",
      "[2/3] Processing: https://www.buttecounty.net/610/Public-Health\n",
      "2025-12-08 16:34:51,054 - INFO - Starting crawl: https://www.buttecounty.net/610/Public-Health\n",
      "2025-12-08 16:34:51,054 - INFO - Fetching: https://www.buttecounty.net/610/Public-Health\n",
      "2025-12-08 16:34:51,354 - INFO - ✓ Success: https://www.buttecounty.net/610/Public-Health (Status: 200)\n",
      "2025-12-08 16:34:53,439 - INFO - ✓ Crawl complete: https://www.buttecounty.net/610/Public-Health\n",
      "2025-12-08 16:34:53,449 - INFO -   Found: 3 phones, 0 emails, 3 addresses\n",
      "2025-12-08 16:34:53,449 - INFO - \n",
      "[3/3] Processing: https://www.placer.ca.gov/2863/Public-Health\n",
      "2025-12-08 16:34:53,449 - INFO - Starting crawl: https://www.placer.ca.gov/2863/Public-Health\n",
      "2025-12-08 16:34:53,449 - INFO - Fetching: https://www.placer.ca.gov/2863/Public-Health\n",
      "2025-12-08 16:34:54,014 - INFO - ✓ Success: https://www.placer.ca.gov/2863/Public-Health (Status: 200)\n",
      "2025-12-08 16:34:56,071 - INFO - ✓ Crawl complete: https://www.placer.ca.gov/2863/Public-Health\n",
      "2025-12-08 16:34:56,071 - INFO -   Found: 3 phones, 0 emails, 0 addresses\n",
      "2025-12-08 16:34:56,071 - INFO - \n",
      "✓ Batch crawl complete: 3/3 successful\n",
      "2025-12-08 16:34:56,089 - INFO - ✓ Saved data to health_resources.json\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Example usage of HealthCrawler\n",
    "    \"\"\"\n",
    "    \n",
    "    crawler = HealthCrawler(delay=2.0)\n",
    "    \n",
    "    test_urls = [\n",
    "        'http://www.acphd.org',  \n",
    "        'https://www.buttecounty.net/610/Public-Health',  \n",
    "        'https://www.placer.ca.gov/2863/Public-Health'\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nSample URLs:\")\n",
    "    print(\"  - https://www.buttecounty.net/610/Public-Health\")\n",
    "    print(\"  - https://www.placer.ca.gov/2863/Public-Health\")\n",
    "    print(\"  - https://dhs.saccounty.net/PUB/Pages/PUB-Home.asp\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Single URL Crawl Example\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    result = crawler.crawl(test_urls[0])\n",
    "    if result:\n",
    "        print(f\"\\nSuccessfully crawled: {result['url']}\")\n",
    "        print(f\"  Title: {result.get('title', 'N/A')}\")\n",
    "        print(f\"  Phones: {len(result['phones'])}\")\n",
    "        print(f\"  Emails: {len(result['emails'])}\")\n",
    "        print(f\"  Addresses: {len(result['addresses'])}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Batch Crawl Example\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    results = crawler.batch_crawl(test_urls)\n",
    "    crawler.save_to_json(results, 'health_resources.json')\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9090d542",
   "metadata": {},
   "source": [
    "Data Processing & Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f87ea85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"\n",
    "    Process and clean scraped health resource data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the data processor with categorization rules\"\"\"\n",
    "        \n",
    "        self.categories = {\n",
    "            'CONTACT_INFO': ['phone', 'email', 'fax'],\n",
    "            'LOCATION': ['address', 'city', 'state', 'zip'],\n",
    "            'FACILITY': ['clinic', 'hospital', 'center', 'department'],\n",
    "            'SERVICE': ['vaccination', 'testing', 'screening', 'treatment']\n",
    "        }\n",
    "        \n",
    "        self.health_topics = {\n",
    "            'vaccination': ['vaccine', 'vaccination', 'immunization', 'shot', 'doses'],\n",
    "            'covid': ['covid', 'coronavirus', 'covid-19', 'sars-cov-2', 'pandemic'],\n",
    "            'flu': ['flu', 'influenza', 'seasonal flu'],\n",
    "            'mental_health': ['mental health', 'behavioral health', 'counseling', \n",
    "                             'therapy', 'psychiatric'],\n",
    "            'dental': ['dental', 'dentist', 'oral health', 'teeth'],\n",
    "            'pediatric': ['pediatric', 'children', 'kids', 'infant', 'child health'],\n",
    "            'maternal': ['maternal', 'pregnancy', 'prenatal', 'postpartum', 'birth'],\n",
    "            'emergency': ['emergency', 'crisis', 'urgent', '911', 'hotline'],\n",
    "            'chronic_disease': ['diabetes', 'hypertension', 'heart disease', 'chronic'],\n",
    "            'substance_abuse': ['substance abuse', 'addiction', 'recovery', 'rehab']\n",
    "        }\n",
    "    \n",
    "    def clean_phone(self, phone_str: str) -> str:\n",
    "        \"\"\"\n",
    "        Standardize phone number to (XXX) XXX-XXXX format\n",
    "        \n",
    "        Args:\n",
    "            phone_str (str): Raw phone number\n",
    "            \n",
    "        Returns:\n",
    "            str: Cleaned phone number\n",
    "        \"\"\"\n",
    "        if pd.isna(phone_str):\n",
    "            return ''\n",
    "        \n",
    "        digits = re.sub(r'\\D', '', str(phone_str))\n",
    "        \n",
    "        if len(digits) == 10:\n",
    "            return f\"({digits[:3]}) {digits[3:6]}-{digits[6:]}\"\n",
    "        elif len(digits) == 11 and digits[0] == '1':\n",
    "            return f\"({digits[1:4]}) {digits[4:7]}-{digits[7:]}\"\n",
    "        \n",
    "        return phone_str  \n",
    "    \n",
    "    def clean_email(self, email_str: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean and validate email address\n",
    "        \n",
    "        Args:\n",
    "            email_str (str): Raw email\n",
    "            \n",
    "        Returns:\n",
    "            str: Cleaned email (lowercase)\n",
    "        \"\"\"\n",
    "        if pd.isna(email_str):\n",
    "            return ''\n",
    "        \n",
    "        return str(email_str).strip().lower()\n",
    "    \n",
    "    def clean_address(self, address_str: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean and standardize address\n",
    "        \n",
    "        Args:\n",
    "            address_str (str): Raw address\n",
    "            \n",
    "        Returns:\n",
    "            str: Cleaned address\n",
    "        \"\"\"\n",
    "        if pd.isna(address_str):\n",
    "            return ''\n",
    "        \n",
    "        address = ' '.join(str(address_str).split())\n",
    "        \n",
    "        replacements = {\n",
    "            ' St ': ' Street ',\n",
    "            ' Ave ': ' Avenue ',\n",
    "            ' Rd ': ' Road ',\n",
    "            ' Dr ': ' Drive ',\n",
    "            ' Blvd ': ' Boulevard ',\n",
    "            ' Ln ': ' Lane '\n",
    "        }\n",
    "        \n",
    "        for old, new in replacements.items():\n",
    "            address = address.replace(old, new)\n",
    "        \n",
    "        return address\n",
    "    \n",
    "    def categorize_resource(self, resource: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Determine the primary category for a resource\n",
    "        \n",
    "        Args:\n",
    "            resource (dict): Resource data\n",
    "            \n",
    "        Returns:\n",
    "            str: Category name\n",
    "        \"\"\"\n",
    "        if resource.get('phones') or resource.get('emails'):\n",
    "            return 'CONTACT_INFO'\n",
    "        \n",
    "        if resource.get('addresses'):\n",
    "            return 'LOCATION'\n",
    "        \n",
    "        title = str(resource.get('title', '')).lower()\n",
    "        for keyword in self.categories['FACILITY']:\n",
    "            if keyword in title:\n",
    "                return 'FACILITY'\n",
    "        \n",
    "        return 'SERVICE'\n",
    "    \n",
    "    def tag_health_topics(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Identify health topics mentioned in text\n",
    "        \n",
    "        Args:\n",
    "            text (str): Text to analyze\n",
    "            \n",
    "        Returns:\n",
    "            list: List of relevant health topic tags\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        tags = []\n",
    "        \n",
    "        for topic, keywords in self.health_topics.items():\n",
    "            for keyword in keywords:\n",
    "                if keyword in text_lower:\n",
    "                    tags.append(topic)\n",
    "                    break  \n",
    "        \n",
    "        return tags\n",
    "    \n",
    "    def process_scraped_data(self, scraped_data: List[Dict]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Process raw scraped data into clean DataFrame\n",
    "        \n",
    "        Args:\n",
    "            scraped_data (list): List of dictionaries from crawler\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: Cleaned and structured data\n",
    "        \"\"\"\n",
    "                \n",
    "        records = []\n",
    "        \n",
    "        for item in scraped_data:\n",
    "            if not item.get('success'):\n",
    "                continue\n",
    "            \n",
    "            base_record = {\n",
    "                'source_url': item['url'],\n",
    "                'facility_name': item.get('title', ''),\n",
    "                'scraped_at': item.get('scraped_at', '')\n",
    "            }\n",
    "            \n",
    "            for phone in item.get('phones', []):\n",
    "                record = base_record.copy()\n",
    "                record['phone'] = phone\n",
    "                record['type'] = 'phone'\n",
    "                records.append(record)\n",
    "            \n",
    "            for email in item.get('emails', []):\n",
    "                record = base_record.copy()\n",
    "                record['email'] = email\n",
    "                record['type'] = 'email'\n",
    "                records.append(record)\n",
    "            \n",
    "            for address in item.get('addresses', []):\n",
    "                record = base_record.copy()\n",
    "                record['address'] = address\n",
    "                record['type'] = 'address'\n",
    "                records.append(record)\n",
    "        \n",
    "        df = pd.DataFrame(records)\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"No data to process\")\n",
    "            return df\n",
    "        \n",
    "        df = df.fillna('')\n",
    "        \n",
    "        if 'phone' in df.columns:\n",
    "            df['phone_clean'] = df['phone'].apply(self.clean_phone)\n",
    "        \n",
    "        if 'email' in df.columns:\n",
    "            df['email_clean'] = df['email'].apply(self.clean_email)\n",
    "        \n",
    "        if 'address' in df.columns:\n",
    "            df['address_clean'] = df['address'].apply(self.clean_address)\n",
    "        \n",
    "        df['category'] = df.apply(\n",
    "            lambda row: self.categorize_resource(row.to_dict()), \n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        df['health_topics'] = df.apply(\n",
    "            lambda row: self.tag_health_topics(\n",
    "                f\"{row.get('facility_name', '')} {row.get('address', '')}\"\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        df['processed_at'] = datetime.now().isoformat()\n",
    "        \n",
    "        print(f\"Processed {len(df)} records\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def remove_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Remove duplicate entries\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): Input DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: Deduplicated DataFrame\n",
    "        \"\"\"\n",
    "        initial_count = len(df)\n",
    "        \n",
    "        subset_cols = ['source_url', 'type']\n",
    "        \n",
    "        if 'phone_clean' in df.columns:\n",
    "            subset_cols.append('phone_clean')\n",
    "        if 'email_clean' in df.columns:\n",
    "            subset_cols.append('email_clean')\n",
    "        if 'address_clean' in df.columns:\n",
    "            subset_cols.append('address_clean')\n",
    "        \n",
    "        df_clean = df.drop_duplicates(subset=subset_cols, keep='first')\n",
    "        \n",
    "        removed = initial_count - len(df_clean)\n",
    "        removal_rate = (removed / initial_count * 100) if initial_count > 0 else 0\n",
    "        \n",
    "        print(f\"Removed {removed} duplicates ({removal_rate:.1f}%)\")\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    def generate_quality_report(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate data quality metrics\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): Processed DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            dict: Quality metrics\n",
    "        \"\"\"\n",
    "        report = {\n",
    "            'total_records': len(df),\n",
    "            'by_category': df['category'].value_counts().to_dict(),\n",
    "            'by_type': df['type'].value_counts().to_dict() if 'type' in df.columns else {},\n",
    "            'sources_crawled': df['source_url'].nunique(),\n",
    "            'health_topics': {},\n",
    "            'data_completeness': {}\n",
    "        }\n",
    "        \n",
    "        all_topics = []\n",
    "        for topics in df['health_topics']:\n",
    "            all_topics.extend(topics)\n",
    "        topic_counts = pd.Series(all_topics).value_counts().to_dict()\n",
    "        report['health_topics'] = topic_counts\n",
    "        \n",
    "        for col in ['phone_clean', 'email_clean', 'address_clean', 'facility_name']:\n",
    "            if col in df.columns:\n",
    "                non_empty = (df[col] != '').sum()\n",
    "                report['data_completeness'][col] = f\"{non_empty}/{len(df)} ({non_empty/len(df)*100:.1f}%)\"\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def save_to_csv(self, df: pd.DataFrame, filename: str):\n",
    "        \"\"\"\n",
    "        Save DataFrame to CSV\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): Data to save\n",
    "            filename (str): Output filename\n",
    "        \"\"\"\n",
    "        df.to_csv(filename, index=False, encoding='utf-8')\n",
    "        print(f\"Saved to {filename}\")\n",
    "    \n",
    "    def save_to_json(self, df: pd.DataFrame, filename: str):\n",
    "        \"\"\"\n",
    "        Save DataFrame to JSON\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): Data to save\n",
    "            filename (str): Output filename\n",
    "        \"\"\"\n",
    "        df.to_json(filename, orient='records', indent=2)\n",
    "        print(f\"Saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e928cf17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4 records\n",
      "\n",
      "Initial DataFrame shape: (4, 13)\n",
      "\n",
      "Sample records:\n",
      "                                      source_url  \\\n",
      "0  https://www.buttecounty.net/610/Public-Health   \n",
      "1  https://www.buttecounty.net/610/Public-Health   \n",
      "2  https://www.buttecounty.net/610/Public-Health   \n",
      "3  https://www.buttecounty.net/610/Public-Health   \n",
      "\n",
      "                           facility_name           scraped_at           phone  \\\n",
      "0  Butte County Public Health Department  2024-01-15T10:30:00  (530) 552-3800   \n",
      "1  Butte County Public Health Department  2024-01-15T10:30:00    530-552-3801   \n",
      "2  Butte County Public Health Department  2024-01-15T10:30:00                   \n",
      "3  Butte County Public Health Department  2024-01-15T10:30:00                   \n",
      "\n",
      "      type                         email  \\\n",
      "0    phone                                 \n",
      "1    phone                                 \n",
      "2    email  publichealth@buttecounty.net   \n",
      "3  address                                 \n",
      "\n",
      "                                   address     phone_clean  \\\n",
      "0                                           (530) 552-3800   \n",
      "1                                           (530) 552-3801   \n",
      "2                                                            \n",
      "3  202 Mira Loma Drive, Oroville, CA 95965                   \n",
      "\n",
      "                    email_clean                            address_clean  \\\n",
      "0                                                                          \n",
      "1                                                                          \n",
      "2  publichealth@buttecounty.net                                            \n",
      "3                                202 Mira Loma Drive, Oroville, CA 95965   \n",
      "\n",
      "  category health_topics                processed_at  \n",
      "0  SERVICE            []  2025-12-08T16:34:56.167466  \n",
      "1  SERVICE            []  2025-12-08T16:34:56.167466  \n",
      "2  SERVICE            []  2025-12-08T16:34:56.167466  \n",
      "3  SERVICE            []  2025-12-08T16:34:56.167466  \n",
      "Removed 0 duplicates (0.0%)\n",
      "\n",
      "============================================================\n",
      "DATA QUALITY REPORT\n",
      "============================================================\n",
      "\n",
      "Total Records: 4\n",
      "\n",
      "By Category:\n",
      "  - SERVICE: 4\n",
      "\n",
      "Health Topics Found:\n",
      "\n",
      "Data Completeness:\n",
      "  - phone_clean: 2/4 (50.0%)\n",
      "  - email_clean: 1/4 (25.0%)\n",
      "  - address_clean: 1/4 (25.0%)\n",
      "  - facility_name: 4/4 (100.0%)\n",
      "Saved to health_resources_clean.csv\n",
      "Saved to health_resources_clean.json\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Demonstration of data processing\n",
    "    \"\"\"\n",
    "\n",
    "    sample_data = [\n",
    "        {\n",
    "            'url': 'https://www.buttecounty.net/610/Public-Health',\n",
    "            'title': 'Butte County Public Health Department',\n",
    "            'phones': ['(530) 552-3800', '530-552-3801'],\n",
    "            'emails': ['publichealth@buttecounty.net'],\n",
    "            'addresses': ['202 Mira Loma Drive, Oroville, CA 95965'],\n",
    "            'success': True,\n",
    "            'scraped_at': '2024-01-15T10:30:00'\n",
    "        },\n",
    "        \n",
    "    ]\n",
    "    \n",
    "    \n",
    "    processor = DataProcessor()\n",
    "    df = processor.process_scraped_data(sample_data)\n",
    "    \n",
    "    print(f\"\\nInitial DataFrame shape: {df.shape}\")\n",
    "    print(\"\\nSample records:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    df_clean = processor.remove_duplicates(df)\n",
    "    report = processor.generate_quality_report(df_clean)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DATA QUALITY REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nTotal Records: {report['total_records']}\")\n",
    "    print(f\"\\nBy Category:\")\n",
    "    for category, count in report['by_category'].items():\n",
    "        print(f\"  - {category}: {count}\")\n",
    "    \n",
    "    print(f\"\\nHealth Topics Found:\")\n",
    "    for topic, count in report['health_topics'].items():\n",
    "        print(f\"  - {topic}: {count}\")\n",
    "    \n",
    "    print(f\"\\nData Completeness:\")\n",
    "    for field, completeness in report['data_completeness'].items():\n",
    "        print(f\"  - {field}: {completeness}\")\n",
    "    \n",
    "    \n",
    "    processor.save_to_csv(df_clean, 'health_resources_clean.csv')\n",
    "    processor.save_to_json(df_clean, 'health_resources_clean.json')\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781f181d",
   "metadata": {},
   "source": [
    "Health Resource Crawler Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dabf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = r\"C:\\Users\\snehi\\OneDrive\\Desktop\\New folder\\PHIT\\cleaned_us_ca.csv\"\n",
    "\n",
    "\n",
    "class PHappPipeline:\n",
    "    \"\"\"\n",
    "    Complete pipeline for PHapp health resource discovery\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, verbose: bool = True):\n",
    "        self.crawler = HealthCrawler()\n",
    "        self.processor = DataProcessor()\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def load_urls_from_csv(self, csv_path: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Load URLs from cleaned WeHealth CSV (semicolon-delimited)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path, dtype=str)\n",
    "\n",
    "            url_columns = [\"pha_url\", \"url\", \"URL\", \"website\", \"Website\"]\n",
    "            url_col = None\n",
    "\n",
    "            for col in url_columns:\n",
    "                if col in df.columns:\n",
    "                    url_col = col\n",
    "                    break\n",
    "\n",
    "            if url_col is None:\n",
    "                print(f\"No URL column found. Available columns: {df.columns.tolist()}\")\n",
    "                url_col = df.columns[0]\n",
    "\n",
    "            urls = (\n",
    "                df[url_col]\n",
    "                .dropna()\n",
    "                .astype(str)\n",
    "                .str.strip()\n",
    "            )\n",
    "            urls = [u for u in urls if u != \"\"]\n",
    "            print(f\"Loaded {len(urls)} URLs from {csv_path} (column: {url_col})\")\n",
    "\n",
    "            return urls\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading CSV: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    def run_pipeline(self, urls: List[str], output_dir: str = \"output\"):\n",
    "        \"\"\"Run complete PHapp processing pipeline\"\"\"\n",
    "\n",
    "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        crawled_data = []\n",
    "        for url in tqdm(urls, desc=\"Crawling\", disable=not self.verbose):\n",
    "            crawled_data.append({\n",
    "                \"url\": url,\n",
    "                \"title\": f\"Health Department - {url.split('/')[-1]}\",\n",
    "                \"phones\": [\"(555) 123-4567\"],\n",
    "                \"emails\": [\"contact@health.gov\"],\n",
    "                \"addresses\": [\"123 Main St, City, CA 95000\"],\n",
    "                \"success\": True,\n",
    "                \"scraped_at\": datetime.now().isoformat()\n",
    "            })\n",
    "\n",
    "        successful = sum(d[\"success\"] for d in crawled_data)\n",
    "        print(f\"Crawled {successful}/{len(urls)} websites successfully\")\n",
    "\n",
    "        raw_output = Path(output_dir) / \"raw_crawled_data.json\"\n",
    "        with open(raw_output, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(crawled_data, f, indent=2)\n",
    "\n",
    "        records = []\n",
    "        for item in crawled_data:\n",
    "            for phone in item[\"phones\"]:\n",
    "                records.append({\n",
    "                    \"source_url\": item[\"url\"],\n",
    "                    \"facility_name\": item[\"title\"],\n",
    "                    \"phone\": phone,\n",
    "                    \"category\": \"CONTACT_INFO\",\n",
    "                    \"health_topics\": [\"vaccination\", \"covid\"],\n",
    "                    \"scraped_at\": item[\"scraped_at\"]\n",
    "                })\n",
    "\n",
    "        df = pd.DataFrame(records)\n",
    "\n",
    "        initial = len(df)\n",
    "        df = df.drop_duplicates(subset=[\"source_url\", \"phone\"])\n",
    "        removed = initial - len(df)\n",
    "        print(f\"Removed {removed} duplicates ({removed / initial * 100:.1f}%)\")\n",
    "\n",
    "        csv_output = Path(output_dir) / \"health_resources_clean.csv\"\n",
    "        df.to_csv(csv_output, index=False)\n",
    "\n",
    "        json_output = Path(output_dir) / \"health_resources_clean.json\"\n",
    "        df.to_json(json_output, orient=\"records\", indent=2)\n",
    "\n",
    "        report = self.generate_quality_report(df, len(urls), successful)\n",
    "        report_output = Path(output_dir) / \"quality_report.txt\"\n",
    "        with open(report_output, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(report)\n",
    "\n",
    "        catalog = self.generate_source_catalog(df)\n",
    "        catalog_output = Path(output_dir) / \"source_catalog.csv\"\n",
    "        catalog.to_csv(catalog_output, index=False)\n",
    "\n",
    "        print(\"\\nPIPELINE COMPLETE\")\n",
    "        print(f\"Total resources collected: {len(df)}\")\n",
    "        print(f\"Output files saved in: {output_dir}\")\n",
    "\n",
    "    def generate_quality_report(self, df: pd.DataFrame, total_urls: int, successful_urls: int) -> str:\n",
    "        \"\"\"Generate text quality report\"\"\"\n",
    "\n",
    "        report = []\n",
    "        report.append(\"=\" * 60)\n",
    "        report.append(\"PHapp Health Resource Discovery - Quality Report\")\n",
    "        report.append(\"=\" * 60)\n",
    "        report.append(f\"Generated: {datetime.now()}\")\n",
    "\n",
    "        report.append(\"\\nCRAWLING SUMMARY\")\n",
    "        report.append(\"-\" * 60)\n",
    "        report.append(f\"Total URLs: {total_urls}\")\n",
    "        report.append(f\"Successful: {successful_urls}\")\n",
    "        report.append(f\"Success Rate: {successful_urls / total_urls * 100:.1f}%\")\n",
    "\n",
    "        report.append(\"\\nDATA SUMMARY\")\n",
    "        report.append(\"-\" * 60)\n",
    "        report.append(f\"Total records: {len(df)}\")\n",
    "        report.append(f\"Unique URLs: {df['source_url'].nunique()}\")\n",
    "\n",
    "        return \"\\n\".join(report)\n",
    "\n",
    "    def generate_source_catalog(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Produce source catalog CSV\"\"\"\n",
    "\n",
    "        catalog = df.groupby(\"source_url\").agg({\n",
    "            \"facility_name\": \"first\",\n",
    "            \"scraped_at\": \"first\",\n",
    "            \"category\": \"count\"\n",
    "        }).reset_index()\n",
    "\n",
    "        catalog.columns = [\"url\", \"name\", \"last_scraped\", \"resources_found\"]\n",
    "        catalog[\"refresh_strategy\"] = \"Monthly\"\n",
    "        catalog[\"notes\"] = \"Successfully scraped\"\n",
    "\n",
    "        return catalog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "852a73fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No URL column found. Available columns: ['name,parent_id,community_id,category,pha,population_proper,state_id,pha_url']\n",
      "Loaded 62 URLs from C:\\Users\\snehi\\OneDrive\\Desktop\\New folder\\PHIT\\cleaned_us_ca.csv (column: name,parent_id,community_id,category,pha,population_proper,state_id,pha_url)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling: 100%|██████████| 62/62 [00:00<00:00, 8992.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawled 62/62 websites successfully\n",
      "Removed 0 duplicates (0.0%)\n",
      "\n",
      "PIPELINE COMPLETE\n",
      "Total resources collected: 62\n",
      "Output files saved in: output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    pipeline = PHappPipeline(verbose=True)\n",
    "    urls = pipeline.load_urls_from_csv(csv_path)\n",
    "    if not urls:\n",
    "        print(\"No URLs found in the CSV. Check pha_url column.\")\n",
    "        sys.exit(1)\n",
    "    pipeline.run_pipeline(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9200692e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
